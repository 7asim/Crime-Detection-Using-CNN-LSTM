{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CNN + LSTM Crime Detection"
      ],
      "metadata": {
        "id": "JaHGBp8xCtkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset\n",
        "https://www.kaggle.com/datasets/odins0n/ucf-crime-dataset"
      ],
      "metadata": {
        "id": "yLb17UGCC6a3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6W8nHjYzCDfx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define constants\n",
        "IMAGE_SIZE = (64, 64)  # Example image size\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 100\n",
        "NUM_CLASSES = 14  # Number of classes\n",
        "MAX_SEQ_LENGTH = 100  # Maximum sequence length for text data\n",
        "\n",
        "# Directory containing subfolders for each class\n",
        "train_dir = 'path/to/train_directory'\n",
        "test_dir = 'path/to/test_directory'"
      ],
      "metadata": {
        "id": "qZquoU3-CMAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preprocessing and augmentation for images\n",
        "image_data_generator = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Load training data from directory\n",
        "train_image_generator = image_data_generator.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=None,  # Since we are using custom data, we set this to None\n",
        "    shuffle=False  # Important for maintaining correspondence with text data\n",
        ")\n",
        "\n",
        "# Load test data from directory\n",
        "test_image_generator = image_data_generator.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=None,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "LKADXrLpCMI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load text data\n",
        "train_texts = []\n",
        "test_texts = []\n",
        "train_labels = train_image_generator.classes\n",
        "test_labels = test_image_generator.classes\n",
        "\n",
        "# Assuming you have functions to load text data from files or other sources\n",
        "def load_text_data(directory):\n",
        "    # Load text data from files in the directory\n",
        "    pass\n",
        "\n",
        "# Load and preprocess text data\n",
        "train_texts = load_text_data(train_dir)\n",
        "test_texts = load_text_data(test_dir)\n",
        "\n",
        "# Tokenize and pad text sequences\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "train_sequences_padded = pad_sequences(train_sequences, maxlen=MAX_SEQ_LENGTH, padding='post', truncating='post')\n",
        "test_sequences_padded = pad_sequences(test_sequences, maxlen=MAX_SEQ_LENGTH, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "FFPGgRe1Cc84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LSTM model for text processing\n",
        "def build_lstm_model(input_shape, vocab_size, embedding_dim):\n",
        "    model = models.Sequential([\n",
        "        layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_shape),\n",
        "        layers.LSTM(64),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Build and compile LSTM model\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size\n",
        "embedding_dim = 64  # Example embedding dimension\n",
        "lstm_model = build_lstm_model(input_shape=MAX_SEQ_LENGTH, vocab_size=vocab_size, embedding_dim=embedding_dim)\n",
        "lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train LSTM model\n",
        "lstm_model.fit(train_sequences_padded, train_labels, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, validation_data=(test_sequences_padded, test_labels))"
      ],
      "metadata": {
        "id": "1kLMA5ggCdCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cnn_model(input_shape, num_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(256, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(512, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dropout(0.5),  # Adding dropout for regularization\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dropout(0.5),  # Adding dropout for regularization\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "35YqbyiICEQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and compile CNN model\n",
        "cnn_model = build_cnn_model(input_shape=(*IMAGE_SIZE, 3))\n",
        "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train CNN model\n",
        "cnn_model.fit(\n",
        "    train_image_generator,\n",
        "    steps_per_epoch=train_image_generator.samples // BATCH_SIZE,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    validation_data=test_image_generator,\n",
        "    validation_steps=test_image_generator.samples // BATCH_SIZE\n",
        ")"
      ],
      "metadata": {
        "id": "CxrV0mcECXwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the models\n",
        "cnn_model.save('crime_detection_cnn_model.h5')\n",
        "lstm_model.save('crime_detection_lstm_model.h5')"
      ],
      "metadata": {
        "id": "t1n2Q3EUCETh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
